{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0659c861-fae6-4e54-9671-aefd84a4f4fc",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "The following Python script is designed to extract textual data from a list of URLs\n",
    "given in an Excel file (\"Input.xlsx\")\n",
    "and perform various text analysis tasks to compute a range of variables.\n",
    "The code uses libraries such as pandas, requests, BeautifulSoup, TextBlob, and syllables to achieve this objective."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c4b750c-428b-4bc3-8ac9-fad03d4aaaa7",
   "metadata": {},
   "source": [
    "# Import Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dfb669ca-c8b8-47e2-9db4-e646d2d81668",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from textblob import TextBlob\n",
    "import syllables\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5b9a09-c58d-4632-aa1f-dcd6ec399ab0",
   "metadata": {},
   "source": [
    "# Step 1: Read the Input Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "583872e7-7d17-4c31-bcc4-2c0a56b87c99",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_data = pd.read_excel(\"Input.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c86d95-db7b-4160-b247-3eb586fe18a1",
   "metadata": {},
   "source": [
    "# note:\n",
    "The script begins by loading the input data from an Excel file (\"Input.xlsx\") using the pandas library. \n",
    "The input data is assumed to contain URLs and corresponding URL_ID values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89017255-9455-4b57-99d6-5e3945dc7562",
   "metadata": {},
   "source": [
    "# Initialize an empty list to store the output data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c1090193-4e90-412b-8a25-5630aa3f91ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "output_data = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6de9eec-1cd3-4a7f-a5be-d5f0f93c66b6",
   "metadata": {},
   "source": [
    "# Step 2: Data Extraction and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9122ddbd-afa9-4b07-a335-db571bffaf91",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for index, row in input_data.iterrows():\n",
    "    url_id = row['URL_ID']\n",
    "    url = row['URL']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924e240f-e5bd-4fbb-b3d7-e19889edbe01",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Send an HTTP request to fetch the webpage content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a3b35e2c-4ab8-4435-b94c-13fe91cb8893",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "response = requests.get(url)\n",
    "if response.status_code == 200:\n",
    "    # Parse the HTML content\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad38037c-f70c-48f7-aeea-cbd071a834e0",
   "metadata": {},
   "source": [
    " # Extract article title and text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e2a46431-9367-4d0b-aa9c-fb0b0e1054f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "title = soup.find('title').get_text()\n",
    "article_text = \" \".join([p.get_text() for p in soup.find_all('p')])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e2dc04-06d8-4a0c-af59-51fe4f20866d",
   "metadata": {},
   "source": [
    " # Perform NLP analysis using TextBlob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e8a12b83-5517-4ba1-8a73-f4ba01f2acc8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "blob = TextBlob(article_text)\n",
    "positive_score = blob.sentiment.polarity\n",
    "negative_score = blob.sentiment.subjectivity\n",
    "polarity_score = blob.sentiment.polarity\n",
    "subjectivity_score = blob.sentiment.subjectivity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fddd3d0-d1c3-45da-ab94-306cad53477e",
   "metadata": {},
   "source": [
    "# Calculate other variables as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dd91eefd-5fb3-4ae5-98d5-35198368f589",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**********************************************************************\n",
      "  Resource \u001b[93mpunkt\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('punkt')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtokenizers/punkt/english.pickle\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\pant/nltk_data'\n",
      "    - 'C:\\\\Users\\\\pant\\\\anaconda3\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\pant\\\\anaconda3\\\\share\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\pant\\\\anaconda3\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\pant\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "    - ''\n",
      "**********************************************************************\n",
      "\n"
     ]
    },
    {
     "ename": "MissingCorpusError",
     "evalue": "\nLooks like you are missing some required data for this feature.\n\nTo download the necessary data, simply run\n\n    python -m textblob.download_corpora\n\nor use the NLTK downloader to download the missing data: http://nltk.org/data.html\nIf this doesn't fix the problem, file an issue at https://github.com/sloria/TextBlob/issues.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\textblob\\decorators.py:35\u001b[0m, in \u001b[0;36mrequires_nltk_corpus.<locals>.decorated\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 35\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\textblob\\tokenizers.py:57\u001b[0m, in \u001b[0;36mSentenceTokenizer.tokenize\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m'''Return a list of sentences.'''\u001b[39;00m\n\u001b[1;32m---> 57\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m nltk\u001b[38;5;241m.\u001b[39mtokenize\u001b[38;5;241m.\u001b[39msent_tokenize(text)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:106\u001b[0m, in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;124;03mReturn a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;124;03musing NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;124;03m:param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m--> 106\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m load(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizers/punkt/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlanguage\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pickle\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    107\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\data.py:750\u001b[0m, in \u001b[0;36mload\u001b[1;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[0;32m    749\u001b[0m \u001b[38;5;66;03m# Load the resource.\u001b[39;00m\n\u001b[1;32m--> 750\u001b[0m opened_resource \u001b[38;5;241m=\u001b[39m _open(resource_url)\n\u001b[0;32m    752\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\data.py:876\u001b[0m, in \u001b[0;36m_open\u001b[1;34m(resource_url)\u001b[0m\n\u001b[0;32m    875\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m protocol \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnltk\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 876\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m find(path_, path \u001b[38;5;241m+\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mopen()\n\u001b[0;32m    877\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    878\u001b[0m     \u001b[38;5;66;03m# urllib might not use mode='rb', so handle this one ourselves:\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/english.pickle\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\pant/nltk_data'\n    - 'C:\\\\Users\\\\pant\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\pant\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\pant\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\pant\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - ''\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mMissingCorpusError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m sentences \u001b[38;5;241m=\u001b[39m blob\u001b[38;5;241m.\u001b[39msentences\n\u001b[0;32m      2\u001b[0m avg_sentence_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(\u001b[38;5;28mlen\u001b[39m(sentence\u001b[38;5;241m.\u001b[39msplit()) \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m sentences) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(sentences)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\textblob\\decorators.py:24\u001b[0m, in \u001b[0;36mcached_property.__get__\u001b[1;34m(self, obj, cls)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n\u001b[1;32m---> 24\u001b[0m value \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc(obj)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m value\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\textblob\\blob.py:639\u001b[0m, in \u001b[0;36mTextBlob.sentences\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    636\u001b[0m \u001b[38;5;129m@cached_property\u001b[39m\n\u001b[0;32m    637\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msentences\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    638\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return list of :class:`Sentence <Sentence>` objects.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 639\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_sentence_objects()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\textblob\\blob.py:683\u001b[0m, in \u001b[0;36mTextBlob._create_sentence_objects\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    680\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m'''Returns a list of Sentence objects from the raw text.\u001b[39;00m\n\u001b[0;32m    681\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m    682\u001b[0m sentence_objects \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m--> 683\u001b[0m sentences \u001b[38;5;241m=\u001b[39m sent_tokenize(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw)\n\u001b[0;32m    684\u001b[0m char_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m  \u001b[38;5;66;03m# Keeps track of character index within the blob\u001b[39;00m\n\u001b[0;32m    685\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences:\n\u001b[0;32m    686\u001b[0m     \u001b[38;5;66;03m# Compute the start and end indices of the sentence\u001b[39;00m\n\u001b[0;32m    687\u001b[0m     \u001b[38;5;66;03m# within the blob\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\textblob\\base.py:64\u001b[0m, in \u001b[0;36mBaseTokenizer.itokenize\u001b[1;34m(self, text, *args, **kwargs)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mitokenize\u001b[39m(\u001b[38;5;28mself\u001b[39m, text, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     58\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return a generator that generates tokens \"on-demand\".\u001b[39;00m\n\u001b[0;32m     59\u001b[0m \n\u001b[0;32m     60\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 0.6.0\u001b[39;00m\n\u001b[0;32m     61\u001b[0m \n\u001b[0;32m     62\u001b[0m \u001b[38;5;124;03m    :rtype: generator\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 64\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (t \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenize(text, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs))\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\textblob\\decorators.py:38\u001b[0m, in \u001b[0;36mrequires_nltk_corpus.<locals>.decorated\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;28mprint\u001b[39m(err)\n\u001b[1;32m---> 38\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MissingCorpusError()\n",
      "\u001b[1;31mMissingCorpusError\u001b[0m: \nLooks like you are missing some required data for this feature.\n\nTo download the necessary data, simply run\n\n    python -m textblob.download_corpora\n\nor use the NLTK downloader to download the missing data: http://nltk.org/data.html\nIf this doesn't fix the problem, file an issue at https://github.com/sloria/TextBlob/issues.\n"
     ]
    }
   ],
   "source": [
    "sentences = blob.sentences\n",
    "avg_sentence_length = sum(len(sentence.split()) for sentence in sentences) / len(sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "da7cf7ab-6364-4286-b94b-bdbd0b578a31",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\pant\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5314d5d-8212-442f-8ae6-48aa8047a7dd",
   "metadata": {},
   "source": [
    "# Calculate other variables as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ab37b9fa-c69a-462f-88cf-dffc306ec2b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sentences = blob.sentences\n",
    "avg_sentence_length = sum(len(sentence.split()) for sentence in sentences) / len(sentences)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b35a912-a8db-4fda-932b-6edb1d01d116",
   "metadata": {},
   "source": [
    " # Count complex words (words with more than 2 syllables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f010899a-81bf-4162-8428-e27d23358ff1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    " complex_word_count = sum(1 for word in blob.words if syllables.estimate(word) > 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85130833-10e2-45c8-9f66-978c4a677cbb",
   "metadata": {},
   "source": [
    " # Count total words and syllables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a9e6f6fa-3d4c-4c6e-ba1d-27a6390047b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "word_count = len(blob.words)\n",
    "syllable_per_word = sum(syllables.estimate(word) for word in blob.words) / word_count\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac31521-0e2d-41bd-b569-7f72c17f9427",
   "metadata": {},
   "source": [
    " # Count personal pronouns (you can customize this list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "96074798-f6ac-4ecc-aa2f-549a5f4bdfb8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "personal_pronouns = sum(1 for word in blob.words if word.lower() in [\"i\", \"me\", \"my\", \"mine\", \"myself\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52940419-ea35-411f-ac95-7f7aafd8a828",
   "metadata": {},
   "source": [
    "# Calculate Fog Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f1c79d7d-deda-4382-ac31-1fd053fa2352",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fog_index = 0.4 * (avg_sentence_length + (complex_word_count / len(sentences)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89decb1a-0632-472b-957e-28cf19bc011f",
   "metadata": {},
   "source": [
    " # Calculate average number of words per sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a918d235-9437-47cd-9772-db9844cda059",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "avg_words_per_sentence = word_count / len(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3696790-d531-4f42-8828-5479417de7d9",
   "metadata": {},
   "source": [
    "    # Calculate average word length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6eb41ccb-d285-4ff6-808b-454b881c8920",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    " avg_word_length = sum(len(word) for word in blob.words) / word_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41741f72-9ce4-4254-a5bd-9313e6f50e04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2d9cab90-e046-400b-b012-2e479ea5af86",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "output_data.append({\n",
    "            'URL_ID': url_id,\n",
    "            'URL': url,\n",
    "            'POSITIVE SCORE': positive_score,\n",
    "            'NEGATIVE SCORE': negative_score,\n",
    "            'POLARITY SCORE': polarity_score,\n",
    "            'SUBJECTIVITY SCORE': subjectivity_score,\n",
    "            'AVG SENTENCE LENGTH': avg_sentence_length,\n",
    "            'PERCENTAGE OF COMPLEX WORDS': (complex_word_count / word_count) * 100,\n",
    "            'FOG INDEX': fog_index,\n",
    "            'AVG NUMBER OF WORDS PER SENTENCE': avg_words_per_sentence,\n",
    "            'COMPLEX WORD COUNT': complex_word_count,\n",
    "            'WORD COUNT': word_count,\n",
    "            'SYLLABLE PER WORD': syllable_per_word,\n",
    "            'PERSONAL PRONOUNS': personal_pronouns,\n",
    "            'AVG WORD LENGTH': avg_word_length,\n",
    "        })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07f1b9b-ea8f-4947-af48-975779a4f359",
   "metadata": {},
   "source": [
    "# Step 3: Create and save the output data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e1db8fe0-f29d-4f19-8cc6-d69dab391e4f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "output_df = pd.DataFrame(output_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b55cb1-cdcd-4b83-b681-e05242104059",
   "metadata": {},
   "source": [
    "# Save the output data to an Excel file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8fd91dfe-2e2d-4ec5-af92-f23e5f382d6e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "output_df.to_excel(\"Output_Data.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4727af1-10c8-43bd-9428-e65022979d6b",
   "metadata": {},
   "source": [
    "# Conclusion:\n",
    "This project showcases the capability of Python and various libraries, including pandas, \n",
    "requests, BeautifulSoup, TextBlob, and syllables, to extract and analyze textual \n",
    "data from web articles. The resulting analysis and metrics can be valuable for understanding the \n",
    "content and sentiment of these articles, making it a valuable tool for data-driven decision-making in content analysis \n",
    "and web scraping tasks.\n",
    "\n",
    "This project can be extended by adding more features or conducting deeper text analysis, \n",
    "depending on specific needs and requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48112f19-4d5e-496e-a490-3103a6df247a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
